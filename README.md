# NLP-HW2

Documentation
The word vectors used in the default solution are not dependant on context and are the same in all situations. But in real life, one word may have different meanings in different sentences. For example, I can refer to the homework description example of the word "dry". So it is obvious that there are lots of words that can be synonym in some situations, but their word vectors are far together.
By using the semantic relations from an ontology like Wordnet and modify the word vectors to use that information, we can make the word vectors more useful for tasks like lexical substitution which depends on knowledge of various senses of the target word.By implementing the baseline in homework, we created a new word vector which is covering more possible similar words to a specific word, and we could improve the accuracy to 53.02 percent.
As we changed the word vector to cover more possible synonyms for a word, we could improve the accuracy to 53 percent. We set the alpha and beta to one to get this result. One possible way for improving the result is to change the alpha and beta and see how the accuracy changes. We tried a couple of different numbers but the best accuracy we got was for the alpha and beta equal to one.
The other approach we thought of was using ELMO which considers the previous words of a word to create the word vector for it. But it was stated in homework we shouldn't use such approaches.
The other approach we thought of was using this paper: "context2vec: Learning Generic Context Embedding with Bidirectional LSTM. Oren Melamud, Jacob Goldberger, Ido Dagan. CoNLL, 2016" to add context to the word vectors, but unfortunately, our implementation couldn't help us to get a better score.
